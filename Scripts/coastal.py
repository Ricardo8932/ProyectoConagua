# -*- coding: utf-8 -*-
"""Coastal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cx7mF_mTc-yA6sdvrbaKK0r7W7A1v_jn

#Coastal

Import the necessary libraries
"""

#Import the necessary libraries
import warnings

# Ignore FutureWarnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import pickle
import joblib
import numpy as np
import matplotlib.pyplot as plt
import plotly.io as pio
import pandas as pd
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix, recall_score, matthews_corrcoef, roc_auc_score, jaccard_score, log_loss
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelBinarizer

"""Load the data set"""

# Load coastal data from Excel file into a DataFrame
df_coastal = pd.read_excel("https://raw.githubusercontent.com/Ricardo8932/ProyectoConagua/main/Database/Coastal/coastal_results.xlsx")

# Creates a new DataFrame for nullity matrix (nm) from the years 2012 to 2021, excluding data for the year 2022
df_nm_2012_2021 = df_coastal[df_coastal['AÑO'] != 2022]

# Creates a new DataFrame for nullity matrix (nm) consisting only of data from the year 2022
df_nm_2022 = df_coastal[df_coastal['AÑO'] == 2022]

# Display the first 5 rows of the coastal DataFrame
df_coastal.head(5)

#Replace ">/<" with ""
columns = df_coastal.columns.values

df_coastal[columns] = df_coastal[columns].replace({'<':''}, regex=True)
df_coastal[columns] = df_coastal[columns].replace({'>':''}, regex=True)

# Check the first 5 rows of the coastal DataFrame to verify if '<' and '>' signs have been removed
df_coastal.head(5)

df_coastal.info()

# Change the data type of contaminant columns from object to float64
df_coastal = df_coastal.astype({'AÑO':'object','TSS':'float64','FC':'float64','ODs%':'float64','ODm%':'float64','ODb%':'float64','FE':'float64','TF15s':'float64','TF15b':'float64'})

# Display summary statistics of the coastal DataFrame
df_coastal.describe()

# Display the shape of the coastal DataFrame (number of rows and columns)
df_coastal.shape

# Remove rows with missing values from the coastal DataFrame
df_coastal = df_coastal.dropna()

# Verify that rows with missing values have been removed from the coastal DataFrame
df_coastal.info()

#poner aqui el codigo que descarge el archivo de excel preprocesado, con los metadatos y contaminantes
df_coastal.to_excel("coastal_preprocessed.xlsx")

# Select the columns you want to include in the heatmap
selected_columns_coastal_2012_2021 = df_nm_2012_2021.columns[6:13]  # Replace with the range of column names you want

# Create a heatmap of missing values for only the selected columns
plt.figure(figsize=(7,3))  # Adjust the size of the figure

# Generates a heat map showing the presence of missing values in selected columns
heatmap = sns.heatmap(df_nm_2012_2021[selected_columns_coastal_2012_2021].isnull(), cbar=False, cmap='Blues_r')

# Rotate the x-axis labels by 90 degrees so they are vertical and increase the font size
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, fontsize=17)

# Remove the y-axis ticks and labels
heatmap.set_yticks([])
heatmap.set_yticklabels([])

plt.title('Coastal (2012-2021)', loc='center', fontsize=20)
plt.xlabel('', loc='center', fontsize=20)
plt.show()

# Select the columns you want to include in the heatmap
selected_columns_coastal_2022 = df_nm_2022.columns[6:13]  # Replace with the range of column names you want

# Create a heatmap of missing values for only the selected columns
plt.figure(figsize=(7,3))  # Adjust the size of the figure

# Generates a heat map showing the presence of missing values in selected columns
heatmap = sns.heatmap(df_nm_2022[selected_columns_coastal_2022].isnull(), cbar=False, cmap='Blues_r')

# Rotate the x-axis labels by 90 degrees so they are vertical and increase the font size
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, fontsize=17)

# Remove the y-axis ticks and labels
heatmap.set_yticks([])
heatmap.set_yticklabels([])

plt.title('Coastal (2022)', loc='center', fontsize=20)
plt.xlabel('', loc='center', fontsize=20)
plt.show()

#Realizar un analisis exploratorio de los datos anteriores, como matriz de nulidad, y grafica de correlacion y otro tipo de gracicas para el analisis exploratorio

"""Functions for the 'quality_' and 'complies_' columns

"""

# Create a new DataFrame 'data_coastal' by assigning NaN values to columns for water quality and compliance for each contaminant
data_coastal = df_coastal.assign(quality_TSS=np.nan,quality_FC=np.nan,quality_ODs=np.nan,quality_ODm=np.nan,quality_ODb=np.nan,quality_FE=np.nan,quality_TF15s=np.nan,quality_TF15b=np.nan,
                                complies_TSS=np.nan,complies_FC=np.nan,complies_ODs=np.nan,complies_ODm=np.nan,complies_ODb=np.nan,complies_FE=np.nan,complies_TF15s=np.nan,complies_TF15b=np.nan)

data_coastal.columns

# Display general information about the new columns added to the data_coastal DataFrame
data_coastal.info()

"""Function for the 'quality_' column"""

#---------------------------quality_TSS---------------------------------------------------------------------------------
col = 'TSS'
conditions  = [ data_coastal[col] <= 25, (data_coastal[col] > 25) & (data_coastal[col] <= 75), (data_coastal[col] > 75) & (data_coastal[col] <= 150), (data_coastal[col] > 150) & (data_coastal[col] <= 400), data_coastal[col] > 400 ]
choices     = [ 'Excelente', 'Buena calidad', 'Aceptable', 'Contaminada', 'Fuertemente contaminada']
data_coastal['quality_TSS'] = np.select(conditions, choices, default=np.nan)

#---------------------------quality_FC-------------------------------------------------------------------
col = 'FC'
conditions  = [ data_coastal[col] <= 100, (data_coastal[col] > 100) & (data_coastal[col] <= 200), (data_coastal[col] > 200) & (data_coastal[col] <= 1000), (data_coastal[col] > 1000) & (data_coastal[col] <= 10000), data_coastal[col] > 10000 ]
choices     = [ 'Excelente', 'Buena calidad', 'Aceptable', 'Contaminada', 'Fuertemente contaminada']
data_coastal['quality_FC'] = np.select(conditions, choices, default=np.nan)

#---------------------------quality_ODs-------------------------------------------------------------------------
col = 'ODs%'
conditions  = [ (data_coastal[col] > 70) & (data_coastal[col] <= 110),
                (((data_coastal[col] > 50) & (data_coastal[col] <= 70))|((data_coastal[col] > 110)&(data_coastal[col] <= 120))),
                (((data_coastal[col] > 30) & (data_coastal[col] <= 50))|((data_coastal[col] > 120)&(data_coastal[col] <= 130))),
                (((data_coastal[col] > 10) & (data_coastal[col] <= 30))|((data_coastal[col] > 130)&(data_coastal[col] <= 150))),
                ((data_coastal[col] <= 10)|(data_coastal[col] > 150)) ]
choices     = [ 'Excelente', 'Buena calidad', 'Aceptable', 'Contaminada', 'Fuertemente contaminada']
data_coastal['quality_ODs'] = np.select(conditions, choices, default=np.nan)


#---------------------------quality_ODm-------------------------------------------------------------------------
col = 'ODm%'
conditions  = [ (data_coastal[col] > 70) & (data_coastal[col] <= 110),
                (((data_coastal[col] > 50) & (data_coastal[col] <= 70))|((data_coastal[col] > 110)&(data_coastal[col] <= 120))),
                (((data_coastal[col] > 30) & (data_coastal[col] <= 50))|((data_coastal[col] > 120)&(data_coastal[col] <= 130))),
                (((data_coastal[col] > 10) & (data_coastal[col] <= 30))|((data_coastal[col] > 130)&(data_coastal[col] <= 150))),
                ((data_coastal[col] <= 10)|(data_coastal[col] > 150)) ]
choices     = [ 'Excelente', 'Buena calidad', 'Aceptable', 'Contaminada', 'Fuertemente contaminada']
data_coastal['quality_ODm'] = np.select(conditions, choices, default=np.nan)

#---------------------------quality_ODb-------------------------------------------------------------------------
col = 'ODb%'
conditions  = [ (data_coastal[col] > 70) & (data_coastal[col] <= 110),
                (((data_coastal[col] > 50) & (data_coastal[col] <= 70))|((data_coastal[col] > 110)&(data_coastal[col] <= 120))),
                (((data_coastal[col] > 30) & (data_coastal[col] <= 50))|((data_coastal[col] > 120)&(data_coastal[col] <= 130))),
                (((data_coastal[col] > 10) & (data_coastal[col] <= 30))|((data_coastal[col] > 130)&(data_coastal[col] <= 150))),
                ((data_coastal[col] <= 10)|(data_coastal[col] > 150)) ]
choices     = [ 'Excelente', 'Buena calidad', 'Aceptable', 'Contaminada', 'Fuertemente contaminada']
data_coastal['quality_ODb'] = np.select(conditions, choices, default=np.nan)

#---------------------------quality_FE--------------------------------------------------------------------------------
col = 'FE'
conditions  = [ data_coastal[col] <= 100, (data_coastal[col] > 100) & (data_coastal[col] <= 200), (data_coastal[col] > 200) & (data_coastal[col] <= 500), data_coastal[col] > 500 ]
choices     = [ 'Excelente', 'Buena calidad', 'Contaminada', 'Fuertemente contaminada']
data_coastal['quality_FE'] = np.select(conditions, choices, default=np.nan)

#---------------------------quality_TF15s---------------------------------------------------------------------
col = 'TF15s'
conditions  = [(data_coastal[col] < 1), (data_coastal[col] >= 1) & (data_coastal[col] <= 1.33), (data_coastal[col] > 1.33) & (data_coastal[col] < 5), data_coastal[col] >= 5 ]
choices     = [ 'No tóxico', 'Toxicidad baja', 'Toxicidad moderada', 'Toxicidad alta' ]
data_coastal['quality_TF15s'] = np.select(conditions, choices, default=np.nan)


#---------------------------quality_TF15b---------------------------------------------------------------------
col = 'TF15b'
conditions  = [(data_coastal[col] < 1), (data_coastal[col] >= 1) & (data_coastal[col] <= 1.33), (data_coastal[col] > 1.33) & (data_coastal[col] < 5), data_coastal[col] >= 5 ]
choices     = [ 'No tóxico', 'Toxicidad baja', 'Toxicidad moderada', 'Toxicidad alta' ]
data_coastal['quality_TF15b'] = np.select(conditions, choices, default=np.nan)

data_coastal.iloc[:5, 14:21] #Columnas tipo de quality_

"""Function for the 'complies_' column"""

#---------------------------complies for quality_TSS-----------------------------------------------------
col = 'quality_TSS'
conditions  = [ data_coastal[col] == 'Excelente', data_coastal[col] == 'Buena calidad', data_coastal[col] == 'Aceptable', data_coastal[col] == 'Contaminada', data_coastal[col] == 'Fuertemente contaminada']
choices     = [ 1, 1, 1, 0, 0 ]
data_coastal['complies_TSS'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_TSS'] = data_coastal['complies_TSS'].astype('int32')

#---------------------------complies for quality_FC-----------------------------------------------------
col = 'quality_FC'
conditions  = [ data_coastal[col] == 'Excelente', data_coastal[col] == 'Buena calidad', data_coastal[col] == 'Aceptable', data_coastal[col] == 'Contaminada', data_coastal[col] == 'Fuertemente contaminada']
choices     = [ 1, 1, 1, 0, 0 ]
data_coastal['complies_FC'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_FC'] = data_coastal['complies_FC'].astype('int32')

#---------------------------complies for quality_ODs-----------------------------------------------------
col = 'quality_ODs'
conditions  = [ data_coastal[col] == 'Excelente', data_coastal[col] == 'Buena calidad', data_coastal[col] == 'Aceptable', data_coastal[col] == 'Contaminada', data_coastal[col] == 'Fuertemente contaminada']
choices     = [ 1, 1, 1, 0, 0 ]
data_coastal['complies_ODs'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_ODs'] = data_coastal['complies_ODs'].astype('int32')

#---------------------------complies for quality_ODm-----------------------------------------------------
col = 'quality_ODm'
conditions  = [ data_coastal[col] == 'Excelente', data_coastal[col] == 'Buena calidad', data_coastal[col] == 'Aceptable', data_coastal[col] == 'Contaminada', data_coastal[col] == 'Fuertemente contaminada']
choices     = [ 1, 1, 1, 0, 0 ]
data_coastal['complies_ODm'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_ODm'] = data_coastal['complies_ODm'].astype('int32')

#---------------------------complies for quality_ODb-----------------------------------------------------
col = 'quality_ODb'
conditions  = [ data_coastal[col] == 'Excelente', data_coastal[col] == 'Buena calidad', data_coastal[col] == 'Aceptable', data_coastal[col] == 'Contaminada', data_coastal[col] == 'Fuertemente contaminada']
choices     = [ 1, 1, 1, 0, 0 ]
data_coastal['complies_ODb'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_ODb'] = data_coastal['complies_ODb'].astype('int32')

#---------------------------complies for quality_FE-----------------------------------------------------
col = 'quality_FE'
conditions  = [ data_coastal[col] == 'Excelente', data_coastal[col] == 'Buena calidad', data_coastal[col] == 'Contaminada', data_coastal[col] == 'Fuertemente contaminada']
choices     = [ 1, 1, 0, 0 ]
data_coastal['complies_FE'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_FE'] = data_coastal['complies_FE'].astype('int32')

#---------------------------complies for quality_TF15s-----------------------------------------------------
col = 'quality_TF15s'
conditions  = [ data_coastal[col] == 'No tóxico', data_coastal[col] == 'Toxicidad baja', data_coastal[col] == 'Toxicidad moderada', data_coastal[col] == 'Toxicidad alta']
choices     = [ 1, 1, 1, 0 ]
data_coastal['complies_TF15s'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_TF15s'] = data_coastal['complies_TF15s'].astype('int32')

#---------------------------complies for quality_TF15b-----------------------------------------------------
col = 'quality_TF15b'
conditions  = [ data_coastal[col] == 'No tóxico', data_coastal[col] == 'Toxicidad baja', data_coastal[col] == 'Toxicidad moderada', data_coastal[col] == 'Toxicidad alta']
choices     = [ 1, 1, 1, 0 ]
data_coastal['complies_TF15b'] = np.select(conditions, choices, default=np.nan)
data_coastal['complies_TF15b'] = data_coastal['complies_TF15b'].astype('int32')

data_coastal.iloc[:5, 22:]

"""Assignment of water quality"""

conditions  = [(data_coastal['complies_TF15s'] == 0) | (data_coastal['complies_TF15b'] == 0) | (data_coastal['complies_FE'] == 0),
               (data_coastal['complies_FC'] == 0) | (data_coastal['complies_TSS'] == 0) | (data_coastal['complies_ODs'] == 0) | (data_coastal['complies_ODm'] == 0) | (data_coastal['complies_ODb'] == 0),
               (data_coastal['complies_FC'] == 1) & (data_coastal['complies_TF15s'] == 1) & (data_coastal['complies_TF15b'] == 1) & (data_coastal['complies_ODs'] == 1) & (data_coastal['complies_ODm'] == 1) & (data_coastal['complies_ODb'] == 1) & (data_coastal['complies_FE'] == 1) & (data_coastal['complies_TSS'] == 1)]
choices     = [2, 1, 0]
data_coastal['Water quality'] = np.select(conditions, choices, default=np.nan)
data_coastal['Water quality'] = data_coastal['Water quality'].astype('int32')

# Creates a new DataFrame for model training from the years 2012 to 2021, excluding data for the year 2022
df_train_coastal = data_coastal[data_coastal['AÑO'] != 2022]

# Creates a new DataFrame for validation consisting only of data from the year 2022
df_val_coastal = data_coastal[data_coastal['AÑO'] == 2022]

# Displays the size of the training DataFrame
df_train_coastal.shape

# Displays the size of the validation DataFrame
df_val_coastal.shape

# Selects columns for model training including contaminant columns and the target variable 'Water quality' for the dataset from 2012 to 2021
df_train_coastal = df_train_coastal.iloc[:, list(range(6, 14)) + [30]]
df_train_coastal.head(5)

# Selects columns for model training including contaminant columns and the target variable 'Water quality' for the dataset of 2022
df_val_coastal = df_val_coastal.iloc[:, list(range(6, 14)) + [30]]
df_val_coastal.head(5)

"""Separation of the dependent variable (y) from the independent variables (X)"""

# Separates features (X_train_coastal) and target variable (y_train_coastal) for model training
X_train_coastal = df_train_coastal.drop(columns=['Water quality'])
y_train_coastal = df_train_coastal['Water quality']

# Separates features (X_val_coastal) and target variable (y_val_coastal) for validation
X_val_coastal = df_val_coastal.drop(columns=['Water quality'])
y_val_coastal = df_val_coastal['Water quality']

# Calculate the count of classes present in the 'Water quality' column
conteo_clases = df_train_coastal['Water quality'].value_counts()
print(conteo_clases)

"""##Unbalanced

###XGBOOST

Split data into training and test set
"""

# Split the data into training and testing sets for XGBoost model training
X_train_ub_coa_xgb, X_test_ub_coa_xgb, y_train_ub_coa_xgb, y_test_ub_coa_xgb = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_ub_coa_xgb = StandardScaler()

# Fit and transform the training data to standardize features
X_train_ub_coa_xgb = scaler_ub_coa_xgb.fit_transform(X_train_ub_coa_xgb)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_ub_coa_xgb = scaler_ub_coa_xgb.transform(X_test_ub_coa_xgb)

"""Hyperparameter Grid for XGBoost Model"""

# Define a dictionary of hyperparameters for tuning a XGBoost model
parameters = {
    'learning_rate': [.01, 0.1, 0.2],
    'max_depth': [10, 30, 50],
    'n_estimators': [100, 200, 300]
}

"""Create a Decision XGBoost model"""

# Initialize XGBoost classifier for coastal model
model_ub_coa_xgb = XGBClassifier()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_ub_coa_xgb = GridSearchCV(model_ub_coa_xgb, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_ub_coa_xgb.fit(X_train_ub_coa_xgb, y_train_ub_coa_xgb)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_ub_coa_xgb = grid_search_ub_coa_xgb.best_estimator_
print(best_model_ub_coa_xgb)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_ub_coa_xgb = grid_search_ub_coa_xgb.best_params_
print(best_params_ub_coa_xgb)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_ub_coa_xgb = grid_search_ub_coa_xgb.best_score_
print(best_score_ub_coa_xgb)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_ub_coa_xgb = best_model_ub_coa_xgb.predict(X_train_ub_coa_xgb)

# Evaluation metrics on the training set
accuracy_train_ub_coa_xgb = accuracy_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb)
precision_train_ub_coa_xgb = precision_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average='weighted')
# Calculate precision for each class
precision_x_class_train_ub_coa_xgb = precision_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average=None)
recall_train_ub_coa_xgb = recall_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average='weighted')
# Calculate recall for each class
recall_x_class_train_ub_coa_xgb = recall_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average=None)
f1_score_train_ub_coa_xgb = f1_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_ub_coa_xgb = f1_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average=None)
jaccard_train_ub_coa_xgb = jaccard_score(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb, average='weighted')
confusion_train_ub_coa_xgb = confusion_matrix(y_train_ub_coa_xgb, y_pred_train_ub_coa_xgb)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_ub_coa_xgb)
print("Precision: ", precision_train_ub_coa_xgb)
print("Precision per class:" , precision_x_class_train_ub_coa_xgb )
print("Recall: ", recall_train_ub_coa_xgb)
print("Recall per class:" , recall_x_class_train_ub_coa_xgb )
print("F1 Score: ", f1_score_train_ub_coa_xgb)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_ub_coa_xgb):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_ub_coa_xgb )
print("Confusion Matrix: ")
print(confusion_train_ub_coa_xgb)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_ub_coa_xgb = best_model_ub_coa_xgb.predict(X_test_ub_coa_xgb)

# Evaluation metrics on the testing set
accuracy_test_ub_coa_xgb = accuracy_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb)
precision_test_ub_coa_xgb = precision_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average='weighted')
# Calculate precision for each class
precision_x_class_test_ub_coa_xgb = precision_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average=None)
recall_test_ub_coa_xgb = recall_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average='weighted')
# Calculate recall for each class
recall_x_class_test_ub_coa_xgb = recall_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average=None)
f1_score_test_ub_coa_xgb = f1_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_ub_coa_xgb = f1_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average=None)
jaccard_test_ub_coa_xgb = jaccard_score(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb, average='weighted')
confusion_test_ub_coa_xgb = confusion_matrix(y_test_ub_coa_xgb, y_pred_test_ub_coa_xgb)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_ub_coa_xgb)
print("Precision: ", precision_test_ub_coa_xgb)
print("Precision per class:" , precision_x_class_test_ub_coa_xgb )
print("Recall: ", recall_test_ub_coa_xgb)
print("Recall per class:" , recall_x_class_test_ub_coa_xgb )
print("F1 Score: ", f1_score_test_ub_coa_xgb)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_ub_coa_xgb):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_ub_coa_xgb )
print("Confusion Matrix: ")
print(confusion_test_ub_coa_xgb)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_ub_coa_xgb = scaler_ub_coa_xgb.transform(X_val_coastal)

# Make predictions on the validation set using the trained xgboost model
y_pred_val_ub_coa_xgb = best_model_ub_coa_xgb.predict(X_val_ub_coa_xgb)

# Evaluation metrics on the validation set
accuracy_val_ub_coa_xgb = accuracy_score(y_val_coastal, y_pred_val_ub_coa_xgb)
precision_val_ub_coa_xgb = precision_score(y_val_coastal, y_pred_val_ub_coa_xgb, average='weighted')
# Calculate precision for each class
precision_x_class_val_ub_coa_xgb = precision_score(y_val_coastal, y_pred_val_ub_coa_xgb, average=None)
recall_val_ub_coa_xgb = recall_score(y_val_coastal, y_pred_val_ub_coa_xgb, average='weighted')
# Calculate recall for each class
recall_x_class_val_ub_coa_xgb = recall_score(y_val_coastal, y_pred_val_ub_coa_xgb, average=None)
f1_score_val_ub_coa_xgb = f1_score(y_val_coastal, y_pred_val_ub_coa_xgb, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_ub_coa_xgb = f1_score(y_val_coastal, y_pred_val_ub_coa_xgb, average=None)
jaccard_val_ub_coa_xgb = jaccard_score(y_val_coastal, y_pred_val_ub_coa_xgb, average='weighted')
confusion_val_ub_coa_xgb = confusion_matrix(y_val_coastal, y_pred_val_ub_coa_xgb)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_ub_coa_xgb)
print("Precision: ", precision_val_ub_coa_xgb)
print("Precision per class:" , precision_x_class_val_ub_coa_xgb )
print("Recall: ", recall_val_ub_coa_xgb)
print("Recall per class:" , recall_x_class_val_ub_coa_xgb )
print("F1 Score: ", f1_score_val_ub_coa_xgb)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_ub_coa_xgb):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_ub_coa_xgb )
print("Confusion Matrix: ")
print(confusion_val_ub_coa_xgb)

# Save the trained XGBoost model to a file
with open('best_model_ub_coa_xgb.pkl', 'wb') as model_file:
    pickle.dump(best_model_ub_coa_xgb, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_ub_coa_xgb.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_ub_coa_xgb, scaler_file)

"""###SVM

Split data into training and test set
"""

X_train_ub_coa_svm, X_test_ub_coa_svm, y_train_ub_coa_svm, y_test_ub_coa_svm = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_ub_coa_svm = StandardScaler()

# Fit and transform the training data to standardize features
X_train_ub_coa_svm = scaler_ub_coa_svm.fit_transform(X_train_ub_coa_svm)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_ub_coa_svm = scaler_ub_coa_svm.transform(X_test_ub_coa_svm)

"""Hyperparameter Grid for SVM Model"""

# Define a dictionary of hyperparameters for tuning a SVM model
parameters = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': [0.1, 1, 'scale', 'auto'],
}

"""Create a Decision SVM model"""

#Create a Decision SVM model
model_ub_coa_svm = SVC()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_ub_coa_svm = GridSearchCV(model_ub_coa_svm, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_ub_coa_svm.fit(X_train_ub_coa_svm, y_train_ub_coa_svm)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_ub_coa_svm = grid_search_ub_coa_svm.best_estimator_
print(best_model_ub_coa_svm)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_ub_coa_svm = grid_search_ub_coa_svm.best_params_
print(best_params_ub_coa_svm)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_ub_coa_svm = grid_search_ub_coa_svm.best_score_
print(best_score_ub_coa_svm)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_ub_coa_svm = best_model_ub_coa_svm.predict(X_train_ub_coa_svm)

# Evaluation metrics on the training set
accuracy_train_ub_coa_svm = accuracy_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm)
precision_train_ub_coa_svm = precision_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average='weighted')
# Calculate precision for each class
precision_x_class_train_ub_coa_svm = precision_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average=None)
recall_train_ub_coa_svm = recall_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average='weighted')
# Calculate recall for each class
recall_x_class_train_ub_coa_svm = recall_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average=None)
f1_score_train_ub_coa_svm = f1_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_ub_coa_svm = f1_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average=None)
jaccard_train_ub_coa_svm = jaccard_score(y_train_ub_coa_svm, y_pred_train_ub_coa_svm, average='weighted')
confusion_train_ub_coa_svm = confusion_matrix(y_train_ub_coa_svm, y_pred_train_ub_coa_svm)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_ub_coa_svm)
print("Precision: ", precision_train_ub_coa_svm)
print("Precision per class:" , precision_x_class_train_ub_coa_svm )
print("Recall: ", recall_train_ub_coa_svm)
print("Recall per class:" , recall_x_class_train_ub_coa_svm )
print("F1 Score: ", f1_score_train_ub_coa_svm)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_ub_coa_svm):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_ub_coa_svm )
print("Confusion Matrix: ")
print(confusion_train_ub_coa_svm)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_ub_coa_svm = best_model_ub_coa_svm.predict(X_test_ub_coa_svm)

# Evaluation metrics on the testing set
accuracy_test_ub_coa_svm = accuracy_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm)
precision_test_ub_coa_svm = precision_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average='weighted')
# Calculate precision for each class
precision_x_class_test_ub_coa_svm = precision_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average=None)
recall_test_ub_coa_svm = recall_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average='weighted')
# Calculate recall for each class
recall_x_class_test_ub_coa_svm = recall_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average=None)
f1_score_test_ub_coa_svm = f1_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_ub_coa_svm = f1_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average=None)
jaccard_test_ub_coa_svm = jaccard_score(y_test_ub_coa_svm, y_pred_test_ub_coa_svm, average='weighted')
confusion_test_ub_coa_svm = confusion_matrix(y_test_ub_coa_svm, y_pred_test_ub_coa_svm)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_ub_coa_svm)
print("Precision: ", precision_test_ub_coa_svm)
print("Precision per class:" , precision_x_class_test_ub_coa_svm )
print("Recall: ", recall_test_ub_coa_svm)
print("Recall per class:" , recall_x_class_test_ub_coa_svm )
print("F1 Score: ", f1_score_test_ub_coa_svm)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_ub_coa_svm):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_ub_coa_svm )
print("Confusion Matrix: ")
print(confusion_test_ub_coa_svm)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_ub_coa_svm = scaler_ub_coa_svm.transform(X_val_coastal)

# Make predictions on the validation set using the trained svm model
y_pred_val_ub_coa_svm = best_model_ub_coa_svm.predict(X_val_ub_coa_svm)

# Evaluation metrics on the validation set
accuracy_val_ub_coa_svm = accuracy_score(y_val_coastal, y_pred_val_ub_coa_svm)
precision_val_ub_coa_svm = precision_score(y_val_coastal, y_pred_val_ub_coa_svm, average='weighted')
# Calculate precision for each class
precision_x_class_val_ub_coa_svm = precision_score(y_val_coastal, y_pred_val_ub_coa_svm, average=None)
recall_val_ub_coa_svm = recall_score(y_val_coastal, y_pred_val_ub_coa_svm, average='weighted')
# Calculate recall for each class
recall_x_class_val_ub_coa_svm = recall_score(y_val_coastal, y_pred_val_ub_coa_svm, average=None)
f1_score_val_ub_coa_svm = f1_score(y_val_coastal, y_pred_val_ub_coa_svm, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_ub_coa_svm = f1_score(y_val_coastal, y_pred_val_ub_coa_svm, average=None)
jaccard_val_ub_coa_svm = jaccard_score(y_val_coastal, y_pred_val_ub_coa_svm, average='weighted')
confusion_val_ub_coa_svm = confusion_matrix(y_val_coastal, y_pred_val_ub_coa_svm)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_ub_coa_svm)
print("Precision: ", precision_val_ub_coa_svm)
print("Precision per class:" , precision_x_class_val_ub_coa_svm )
print("Recall: ", recall_val_ub_coa_svm)
print("Recall per class:" , recall_x_class_val_ub_coa_svm )
print("F1 Score: ", f1_score_val_ub_coa_svm)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_ub_coa_svm):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_ub_coa_svm )
print("Confusion Matrix: ")
print(confusion_val_ub_coa_svm)

# Save the trained SVM model to a file
with open('best_model_ub_coa_svm.pkl', 'wb') as model_file:
    pickle.dump(best_model_ub_coa_svm, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_ub_coa_svm.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_ub_coa_svm, scaler_file)

"""###KNN

Split data into training and test set
"""

X_train_ub_coa_knn, X_test_ub_coa_knn, y_train_ub_coa_knn, y_test_ub_coa_knn = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_ub_coa_knn = StandardScaler()

# Fit and transform the training data to standardize features
X_train_ub_coa_knn = scaler_ub_coa_knn.fit_transform(X_train_ub_coa_knn)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_ub_coa_knn = scaler_ub_coa_knn.transform(X_test_ub_coa_knn)

"""Hyperparameter Grid for KNN Model"""

# Define a dictionary of hyperparameters for tuning a KNN model
parameters = {
    'n_neighbors': [3, 5, 7, 9],  # Número de vecinos
    'weights': ['uniform', 'distance'],  # Tipo de ponderación
    'p': [1, 2]  # Parámetro para la distancia (1 para la distancia de Manhattan, 2 para la Euclidiana)
}

"""Create a Decision KNN model"""

model_ub_coa_knn = KNeighborsClassifier()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_ub_coa_knn = GridSearchCV(model_ub_coa_knn, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_ub_coa_knn.fit(X_train_ub_coa_knn, y_train_ub_coa_knn)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_ub_coa_knn = grid_search_ub_coa_knn.best_estimator_
print(best_model_ub_coa_knn)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_ub_coa_knn = grid_search_ub_coa_knn.best_params_
print(best_params_ub_coa_knn)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_ub_coa_knn = grid_search_ub_coa_knn.best_score_
print(best_score_ub_coa_knn)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_ub_coa_knn = best_model_ub_coa_knn.predict(X_train_ub_coa_knn)

# Evaluation metrics on the training set
accuracy_train_ub_coa_knn = accuracy_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn)
precision_train_ub_coa_knn = precision_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average='weighted')
# Calculate precision for each class
precision_x_class_train_ub_coa_knn = precision_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average=None)
recall_train_ub_coa_knn = recall_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average='weighted')
# Calculate recall for each class
recall_x_class_train_ub_coa_knn = recall_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average=None)
f1_score_train_ub_coa_knn = f1_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_ub_coa_knn = f1_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average=None)
jaccard_train_ub_coa_knn = jaccard_score(y_train_ub_coa_knn, y_pred_train_ub_coa_knn, average='weighted')
confusion_train_ub_coa_knn = confusion_matrix(y_train_ub_coa_knn, y_pred_train_ub_coa_knn)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_ub_coa_knn)
print("Precision: ", precision_train_ub_coa_knn)
print("Precision per class:" , precision_x_class_train_ub_coa_knn )
print("Recall: ", recall_train_ub_coa_knn)
print("Recall per class:" , recall_x_class_train_ub_coa_knn )
print("F1 Score: ", f1_score_train_ub_coa_knn)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_ub_coa_knn):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_ub_coa_knn )
print("Confusion Matrix: ")
print(confusion_train_ub_coa_knn)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_ub_coa_knn = best_model_ub_coa_knn.predict(X_test_ub_coa_knn)

# Evaluation metrics on the testing set
accuracy_test_ub_coa_knn = accuracy_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn)
precision_test_ub_coa_knn = precision_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average='weighted')
# Calculate precision for each class
precision_x_class_test_ub_coa_knn = precision_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average=None)
recall_test_ub_coa_knn = recall_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average='weighted')
# Calculate recall for each class
recall_x_class_test_ub_coa_knn = recall_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average=None)
f1_score_test_ub_coa_knn = f1_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_ub_coa_knn = f1_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average=None)
jaccard_test_ub_coa_knn = jaccard_score(y_test_ub_coa_knn, y_pred_test_ub_coa_knn, average='weighted')
confusion_test_ub_coa_knn = confusion_matrix(y_test_ub_coa_knn, y_pred_test_ub_coa_knn)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_ub_coa_knn)
print("Precision: ", precision_test_ub_coa_knn)
print("Precision per class:" , precision_x_class_test_ub_coa_knn )
print("Recall: ", recall_test_ub_coa_knn)
print("Recall per class:" , recall_x_class_test_ub_coa_knn )
print("F1 Score: ", f1_score_test_ub_coa_knn)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_ub_coa_knn):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_ub_coa_knn )
print("Confusion Matrix: ")
print(confusion_test_ub_coa_knn)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_ub_coa_knn = scaler_ub_coa_knn.transform(X_val_coastal)

# Make predictions on the validation set using the trained knn model
y_pred_val_ub_coa_knn = best_model_ub_coa_knn.predict(X_val_ub_coa_knn)

# Evaluation metrics on the validation set
accuracy_val_ub_coa_knn = accuracy_score(y_val_coastal, y_pred_val_ub_coa_knn)
precision_val_ub_coa_knn = precision_score(y_val_coastal, y_pred_val_ub_coa_knn, average='weighted')
# Calculate precision for each class
precision_x_class_val_ub_coa_knn = precision_score(y_val_coastal, y_pred_val_ub_coa_knn, average=None)
recall_val_ub_coa_knn = recall_score(y_val_coastal, y_pred_val_ub_coa_knn, average='weighted')
# Calculate recall for each class
recall_x_class_val_ub_coa_knn = recall_score(y_val_coastal, y_pred_val_ub_coa_knn, average=None)
f1_score_val_ub_coa_knn = f1_score(y_val_coastal, y_pred_val_ub_coa_knn, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_ub_coa_knn = f1_score(y_val_coastal, y_pred_val_ub_coa_knn, average=None)
jaccard_val_ub_coa_knn = jaccard_score(y_val_coastal, y_pred_val_ub_coa_knn, average='weighted')
confusion_val_ub_coa_knn = confusion_matrix(y_val_coastal, y_pred_val_ub_coa_knn)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_ub_coa_knn)
print("Precision: ", precision_val_ub_coa_knn)
print("Precision per class:" , precision_x_class_val_ub_coa_knn )
print("Recall: ", recall_val_ub_coa_knn)
print("Recall per class:" , recall_x_class_val_ub_coa_knn )
print("F1 Score: ", f1_score_val_ub_coa_knn)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_ub_coa_knn):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_ub_coa_knn )
print("Confusion Matrix: ")
print(confusion_val_ub_coa_knn)

# Save the trained KNN model to a file
with open('best_model_ub_coa_knn.pkl', 'wb') as model_file:
    pickle.dump(best_model_ub_coa_knn, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_ub_coa_knn.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_ub_coa_knn, scaler_file)

"""###DT

Split data into training and test set
"""

X_train_ub_coa_dt, X_test_ub_coa_dt, y_train_ub_coa_dt, y_test_ub_coa_dt = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_ub_coa_dt = StandardScaler()

# Fit and transform the training data to standardize features
X_train_ub_coa_dt = scaler_ub_coa_dt.fit_transform(X_train_ub_coa_dt)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_ub_coa_dt = scaler_ub_coa_dt.transform(X_test_ub_coa_dt)

"""Hyperparameter Grid for DT Model"""

# Define a dictionary of hyperparameters for tuning a DT model
parameters = {
    'criterion': ['gini', 'entropy'],  # Criterio para la división de nodos
    'max_depth': [None, 10, 20, 30, 40, 50],  # Profundidad máxima del árbol
    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras para dividir un nodo
    'min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en una hoja
}

"""Create a Decision DT model"""

model_ub_coa_dt = DecisionTreeClassifier()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_ub_coa_dt = GridSearchCV(model_ub_coa_dt, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_ub_coa_dt.fit(X_train_ub_coa_dt, y_train_ub_coa_dt)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_ub_coa_dt = grid_search_ub_coa_dt.best_estimator_
print(best_model_ub_coa_dt)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_ub_coa_dt = grid_search_ub_coa_dt.best_params_
print(best_params_ub_coa_dt)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_ub_coa_dt = grid_search_ub_coa_dt.best_score_
print(best_score_ub_coa_dt)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_ub_coa_dt = best_model_ub_coa_dt.predict(X_train_ub_coa_dt)

# Evaluation metrics on the training set
accuracy_train_ub_coa_dt = accuracy_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt)
precision_train_ub_coa_dt = precision_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average='weighted')
# Calculate precision for each class
precision_x_class_train_ub_coa_dt = precision_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average=None)
recall_train_ub_coa_dt = recall_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average='weighted')
# Calculate recall for each class
recall_x_class_train_ub_coa_dt = recall_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average=None)
f1_score_train_ub_coa_dt = f1_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_ub_coa_dt = f1_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average=None)
jaccard_train_ub_coa_dt = jaccard_score(y_train_ub_coa_dt, y_pred_train_ub_coa_dt, average='weighted')
confusion_train_ub_coa_dt = confusion_matrix(y_train_ub_coa_dt, y_pred_train_ub_coa_dt)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_ub_coa_dt)
print("Precision: ", precision_train_ub_coa_dt)
print("Precision per class:" , precision_x_class_train_ub_coa_dt )
print("Recall: ", recall_train_ub_coa_dt)
print("Recall per class:" , recall_x_class_train_ub_coa_dt )
print("F1 Score: ", f1_score_train_ub_coa_dt)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_ub_coa_dt):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_ub_coa_dt )
print("Confusion Matrix: ")
print(confusion_train_ub_coa_dt)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_ub_coa_dt = best_model_ub_coa_dt.predict(X_test_ub_coa_dt)

# Evaluation metrics on the testing set
accuracy_test_ub_coa_dt = accuracy_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt)
precision_test_ub_coa_dt = precision_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average='weighted')
# Calculate precision for each class
precision_x_class_test_ub_coa_dt = precision_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average=None)
recall_test_ub_coa_dt = recall_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average='weighted')
# Calculate recall for each class
recall_x_class_test_ub_coa_dt = recall_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average=None)
f1_score_test_ub_coa_dt = f1_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_ub_coa_dt = f1_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average=None)
jaccard_test_ub_coa_dt = jaccard_score(y_test_ub_coa_dt, y_pred_test_ub_coa_dt, average='weighted')
confusion_test_ub_coa_dt = confusion_matrix(y_test_ub_coa_dt, y_pred_test_ub_coa_dt)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_ub_coa_dt)
print("Precision: ", precision_test_ub_coa_dt)
print("Precision per class:" , precision_x_class_test_ub_coa_dt )
print("Recall: ", recall_test_ub_coa_dt)
print("Recall per class:" , recall_x_class_test_ub_coa_dt )
print("F1 Score: ", f1_score_test_ub_coa_dt)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_ub_coa_dt):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_ub_coa_dt )
print("Confusion Matrix: ")
print(confusion_test_ub_coa_dt)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_ub_coa_dt = scaler_ub_coa_dt.transform(X_val_coastal)

# Make predictions on the validation set using the trained dt model
y_pred_val_ub_coa_dt = best_model_ub_coa_dt.predict(X_val_ub_coa_dt)

# Evaluation metrics on the validation set
accuracy_val_ub_coa_dt = accuracy_score(y_val_coastal, y_pred_val_ub_coa_dt)
precision_val_ub_coa_dt = precision_score(y_val_coastal, y_pred_val_ub_coa_dt, average='weighted')
# Calculate precision for each class
precision_x_class_val_ub_coa_dt = precision_score(y_val_coastal, y_pred_val_ub_coa_dt, average=None)
recall_val_ub_coa_dt = recall_score(y_val_coastal, y_pred_val_ub_coa_dt, average='weighted')
# Calculate recall for each class
recall_x_class_val_ub_coa_dt = recall_score(y_val_coastal, y_pred_val_ub_coa_dt, average=None)
f1_score_val_ub_coa_dt = f1_score(y_val_coastal, y_pred_val_ub_coa_dt, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_ub_coa_dt = f1_score(y_val_coastal, y_pred_val_ub_coa_dt, average=None)
jaccard_val_ub_coa_dt = jaccard_score(y_val_coastal, y_pred_val_ub_coa_dt, average='weighted')
confusion_val_ub_coa_dt = confusion_matrix(y_val_coastal, y_pred_val_ub_coa_dt)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_ub_coa_dt)
print("Precision: ", precision_val_ub_coa_dt)
print("Precision per class:" , precision_x_class_val_ub_coa_dt )
print("Recall: ", recall_val_ub_coa_dt)
print("Recall per class:" , recall_x_class_val_ub_coa_dt )
print("F1 Score: ", f1_score_val_ub_coa_dt)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_ub_coa_dt):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_ub_coa_dt )
print("Confusion Matrix: ")
print(confusion_val_ub_coa_dt)

# Save the trained DT model to a file
with open('best_model_ub_coa_dt.pkl', 'wb') as model_file:
    pickle.dump(best_model_ub_coa_dt, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_ub_coa_dt.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_ub_coa_dt, scaler_file)

"""###MLR

Split data into training and test set
"""

X_train_ub_coa_mlr, X_test_ub_coa_mlr, y_train_ub_coa_mlr, y_test_ub_coa_mlr = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_ub_coa_mlr = StandardScaler()

# Fit and transform the training data to standardize features
X_train_ub_coa_mlr = scaler_ub_coa_mlr.fit_transform(X_train_ub_coa_mlr)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_ub_coa_mlr = scaler_ub_coa_mlr.transform(X_test_ub_coa_mlr)

"""Hyperparameter Grid for MLR Model"""

# Define a dictionary of hyperparameters for tuning a MLR model
parameters = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Parámetro de regularización
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  # Método de optimización
}

"""Create a Decision MLR model"""

model_ub_coa_mlr = LogisticRegression(max_iter=1000)  # Usamos max_iter para evitar advertencias

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_ub_coa_mlr = GridSearchCV(model_ub_coa_mlr, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_ub_coa_mlr.fit(X_train_ub_coa_mlr, y_train_ub_coa_mlr)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_ub_coa_mlr = grid_search_ub_coa_mlr.best_estimator_
print(best_model_ub_coa_mlr)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_ub_coa_mlr = grid_search_ub_coa_mlr.best_params_
print(best_params_ub_coa_mlr)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_ub_coa_mlr = grid_search_ub_coa_mlr.best_score_
print(best_score_ub_coa_mlr)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_ub_coa_mlr = best_model_ub_coa_mlr.predict(X_train_ub_coa_mlr)

# Evaluation metrics on the training set
accuracy_train_ub_coa_mlr = accuracy_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr)
precision_train_ub_coa_mlr = precision_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average='weighted')
# Calculate precision for each class
precision_x_class_train_ub_coa_mlr = precision_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average=None)
recall_train_ub_coa_mlr = recall_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average='weighted')
# Calculate recall for each class
recall_x_class_train_ub_coa_mlr = recall_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average=None)
f1_score_train_ub_coa_mlr = f1_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_ub_coa_mlr = f1_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average=None)
jaccard_train_ub_coa_mlr = jaccard_score(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr, average='weighted')
confusion_train_ub_coa_mlr = confusion_matrix(y_train_ub_coa_mlr, y_pred_train_ub_coa_mlr)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_ub_coa_mlr)
print("Precision: ", precision_train_ub_coa_mlr)
print("Precision per class:" , precision_x_class_train_ub_coa_mlr )
print("Recall: ", recall_train_ub_coa_mlr)
print("Recall per class:" , recall_x_class_train_ub_coa_mlr )
print("F1 Score: ", f1_score_train_ub_coa_mlr)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_ub_coa_mlr):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_ub_coa_mlr )
print("Confusion Matrix: ")
print(confusion_train_ub_coa_mlr)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_ub_coa_mlr = best_model_ub_coa_mlr.predict(X_test_ub_coa_mlr)

# Evaluation metrics on the testing set
accuracy_test_ub_coa_mlr = accuracy_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr)
precision_test_ub_coa_mlr = precision_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average='weighted')
# Calculate precision for each class
precision_x_class_test_ub_coa_mlr = precision_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average=None)
recall_test_ub_coa_mlr = recall_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average='weighted')
# Calculate recall for each class
recall_x_class_test_ub_coa_mlr = recall_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average=None)
f1_score_test_ub_coa_mlr = f1_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_ub_coa_mlr = f1_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average=None)
jaccard_test_ub_coa_mlr = jaccard_score(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr, average='weighted')
confusion_test_ub_coa_mlr = confusion_matrix(y_test_ub_coa_mlr, y_pred_test_ub_coa_mlr)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_ub_coa_mlr)
print("Precision: ", precision_test_ub_coa_mlr)
print("Precision per class:" , precision_x_class_test_ub_coa_mlr )
print("Recall: ", recall_test_ub_coa_mlr)
print("Recall per class:" , recall_x_class_test_ub_coa_mlr )
print("F1 Score: ", f1_score_test_ub_coa_mlr)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_ub_coa_mlr):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_ub_coa_mlr )
print("Confusion Matrix: ")
print(confusion_test_ub_coa_mlr)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_ub_coa_mlr = scaler_ub_coa_mlr.transform(X_val_coastal)

# Make predictions on the validation set using the trained mlr model
y_pred_val_ub_coa_mlr = best_model_ub_coa_mlr.predict(X_val_ub_coa_mlr)

# Evaluation metrics on the validation set
accuracy_val_ub_coa_mlr = accuracy_score(y_val_coastal, y_pred_val_ub_coa_mlr)
precision_val_ub_coa_mlr = precision_score(y_val_coastal, y_pred_val_ub_coa_mlr, average='weighted')
# Calculate precision for each class
precision_x_class_val_ub_coa_mlr = precision_score(y_val_coastal, y_pred_val_ub_coa_mlr, average=None)
recall_val_ub_coa_mlr = recall_score(y_val_coastal, y_pred_val_ub_coa_mlr, average='weighted')
# Calculate recall for each class
recall_x_class_val_ub_coa_mlr = recall_score(y_val_coastal, y_pred_val_ub_coa_mlr, average=None)
f1_score_val_ub_coa_mlr = f1_score(y_val_coastal, y_pred_val_ub_coa_mlr, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_ub_coa_mlr = f1_score(y_val_coastal, y_pred_val_ub_coa_mlr, average=None)
jaccard_val_ub_coa_mlr = jaccard_score(y_val_coastal, y_pred_val_ub_coa_mlr, average='weighted')
confusion_val_ub_coa_mlr = confusion_matrix(y_val_coastal, y_pred_val_ub_coa_mlr)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_ub_coa_mlr)
print("Precision: ", precision_val_ub_coa_mlr)
print("Precision per class:" , precision_x_class_val_ub_coa_mlr )
print("Recall: ", recall_val_ub_coa_mlr)
print("Recall per class:" , recall_x_class_val_ub_coa_mlr )
print("F1 Score: ", f1_score_val_ub_coa_mlr)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_ub_coa_mlr):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_ub_coa_mlr )
print("Confusion Matrix: ")
print(confusion_val_ub_coa_mlr)

# Save the trained MLR model to a file
with open('best_model_ub_coa_mlr.pkl', 'wb') as model_file:
    pickle.dump(best_model_ub_coa_mlr, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_ub_coa_mlr.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_ub_coa_mlr, scaler_file)

"""##Balanced

Apply random subsampling
"""

print('Before class balancing: ', Counter(y_train_coastal))
print("")
rus = RandomUnderSampler(random_state=42)
X_train_coa_b, y_train_coa_b = rus.fit_resample(X_train_coastal, y_train_coastal)
print('After the balancing y:', Counter(y_train_coa_b))
print("")
print('After the balancing X:', Counter(X_train_coa_b))

"""###XGBOOST

Split data into training and test set
"""

X_train_b_coa_xgb, X_test_b_coa_xgb, y_train_b_coa_xgb, y_test_b_coa_xgb = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_b_coa_xgb = StandardScaler()

# Fit and transform the training data to standardize features
X_train_b_coa_xgb = scaler_b_coa_xgb.fit_transform(X_train_b_coa_xgb)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_b_coa_xgb = scaler_b_coa_xgb.transform(X_test_b_coa_xgb)

"""Hyperparameter Grid for XGBoost Model"""

# Define a dictionary of hyperparameters for tuning a XGBoost model
parameters = {
    'learning_rate': [.01, 0.1, 0.2],
    'max_depth': [10, 30, 50],
    'n_estimators': [100, 200, 300]
}

"""Create a Decision XGBoost model"""

model_b_coa_xgb = XGBClassifier()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_b_coa_xgb = GridSearchCV(model_b_coa_xgb, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_b_coa_xgb.fit(X_train_b_coa_xgb, y_train_b_coa_xgb)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_b_coa_xgb = grid_search_b_coa_xgb.best_estimator_
print(best_model_b_coa_xgb)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_b_coa_xgb = grid_search_b_coa_xgb.best_params_
print(best_params_b_coa_xgb)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_b_coa_xgb = grid_search_b_coa_xgb.best_score_
print(best_score_b_coa_xgb)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_b_coa_xgb = best_model_b_coa_xgb.predict(X_train_b_coa_xgb)

# Evaluation metrics on the training set
accuracy_train_b_coa_xgb = accuracy_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb)
precision_train_b_coa_xgb = precision_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average='weighted')
# Calculate precision for each class
precision_x_class_train_b_coa_xgb = precision_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average=None)
recall_train_b_coa_xgb = recall_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average='weighted')
# Calculate recall for each class
recall_x_class_train_b_coa_xgb = recall_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average=None)
f1_score_train_b_coa_xgb = f1_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_b_coa_xgb = f1_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average=None)
jaccard_train_b_coa_xgb = jaccard_score(y_train_b_coa_xgb, y_pred_train_b_coa_xgb, average='weighted')
confusion_train_b_coa_xgb = confusion_matrix(y_train_b_coa_xgb, y_pred_train_b_coa_xgb)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_b_coa_xgb)
print("Precision: ", precision_train_b_coa_xgb)
print("Precision per class:" , precision_x_class_train_b_coa_xgb )
print("Recall: ", recall_train_b_coa_xgb)
print("Recall per class:" , recall_x_class_train_b_coa_xgb )
print("F1 Score: ", f1_score_train_b_coa_xgb)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_b_coa_xgb):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_b_coa_xgb )
print("Confusion Matrix: ")
print(confusion_train_b_coa_xgb)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_b_coa_xgb = best_model_b_coa_xgb.predict(X_test_b_coa_xgb)

# Evaluation metrics on the testing set
accuracy_test_b_coa_xgb = accuracy_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb)
precision_test_b_coa_xgb = precision_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average='weighted')
# Calculate precision for each class
precision_x_class_test_b_coa_xgb = precision_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average=None)
recall_test_b_coa_xgb = recall_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average='weighted')
# Calculate recall for each class
recall_x_class_test_b_coa_xgb = recall_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average=None)
f1_score_test_b_coa_xgb = f1_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_b_coa_xgb = f1_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average=None)
jaccard_test_b_coa_xgb = jaccard_score(y_test_b_coa_xgb, y_pred_test_b_coa_xgb, average='weighted')
confusion_test_b_coa_xgb = confusion_matrix(y_test_b_coa_xgb, y_pred_test_b_coa_xgb)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_b_coa_xgb)
print("Precision: ", precision_test_b_coa_xgb)
print("Precision per class:" , precision_x_class_test_b_coa_xgb )
print("Recall: ", recall_test_b_coa_xgb)
print("Recall per class:" , recall_x_class_test_b_coa_xgb )
print("F1 Score: ", f1_score_test_b_coa_xgb)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_b_coa_xgb):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_b_coa_xgb )
print("Confusion Matrix: ")
print(confusion_test_b_coa_xgb)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_b_coa_xgb = scaler_b_coa_xgb.transform(X_val_coastal)

# Make predictions on the validation set using the trained xgboost model
y_pred_val_b_coa_xgb = best_model_b_coa_xgb.predict(X_val_b_coa_xgb)

# Evaluation metrics on the validation set
accuracy_val_b_coa_xgb = accuracy_score(y_val_coastal, y_pred_val_b_coa_xgb)
precision_val_b_coa_xgb = precision_score(y_val_coastal, y_pred_val_b_coa_xgb, average='weighted')
# Calculate precision for each class
precision_x_class_val_b_coa_xgb = precision_score(y_val_coastal, y_pred_val_b_coa_xgb, average=None)
recall_val_b_coa_xgb = recall_score(y_val_coastal, y_pred_val_b_coa_xgb, average='weighted')
# Calculate recall for each class
recall_x_class_val_b_coa_xgb = recall_score(y_val_coastal, y_pred_val_b_coa_xgb, average=None)
f1_score_val_b_coa_xgb = f1_score(y_val_coastal, y_pred_val_b_coa_xgb, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_b_coa_xgb = f1_score(y_val_coastal, y_pred_val_b_coa_xgb, average=None)
jaccard_val_b_coa_xgb = jaccard_score(y_val_coastal, y_pred_val_b_coa_xgb, average='weighted')
confusion_val_b_coa_xgb = confusion_matrix(y_val_coastal, y_pred_val_b_coa_xgb)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_b_coa_xgb)
print("Precision: ", precision_val_b_coa_xgb)
print("Precision per class:" , precision_x_class_val_b_coa_xgb )
print("Recall: ", recall_val_b_coa_xgb)
print("Recall per class:" , recall_x_class_val_b_coa_xgb )
print("F1 Score: ", f1_score_val_b_coa_xgb)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_b_coa_xgb):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_b_coa_xgb )
print("Confusion Matrix: ")
print(confusion_val_b_coa_xgb)

# Save the trained XGBoost model to a file
with open('best_model_b_coa_xgb.pkl', 'wb') as model_file:
    pickle.dump(best_model_b_coa_xgb, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_b_coa_xgb.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_b_coa_xgb, scaler_file)

"""###SVM

Split data into training and test set
"""

X_train_b_coa_svm, X_test_b_coa_svm, y_train_b_coa_svm, y_test_b_coa_svm = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for Coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_b_coa_svm = StandardScaler()

# Fit and transform the training data to standardize features
X_train_b_coa_svm = scaler_b_coa_svm.fit_transform(X_train_b_coa_svm)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_b_coa_svm = scaler_b_coa_svm.transform(X_test_b_coa_svm)

"""Hyperparameter Grid for SVM Model"""

# Define a dictionary of hyperparameters for tuning a SVM model
parameters = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': [0.1, 1, 'scale', 'auto'],
}

"""Create a Decision SVM model"""

#Create a Decision SVM model
model_b_coa_svm = SVC()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_b_coa_svm = GridSearchCV(model_b_coa_svm, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_b_coa_svm.fit(X_train_b_coa_svm, y_train_b_coa_svm)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_b_coa_svm = grid_search_b_coa_svm.best_estimator_
print(best_model_b_coa_svm)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_b_coa_svm = grid_search_b_coa_svm.best_params_
print(best_params_b_coa_svm)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_b_coa_svm = grid_search_b_coa_svm.best_score_
print(best_score_b_coa_svm)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_b_coa_svm = best_model_b_coa_svm.predict(X_train_b_coa_svm)

# Evaluation metrics on the training set
accuracy_train_b_coa_svm = accuracy_score(y_train_b_coa_svm, y_pred_train_b_coa_svm)
precision_train_b_coa_svm = precision_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average='weighted')
# Calculate precision for each class
precision_x_class_train_b_coa_svm = precision_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average=None)
recall_train_b_coa_svm = recall_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average='weighted')
# Calculate recall for each class
recall_x_class_train_b_coa_svm = recall_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average=None)
f1_score_train_b_coa_svm = f1_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_b_coa_svm = f1_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average=None)
jaccard_train_b_coa_svm = jaccard_score(y_train_b_coa_svm, y_pred_train_b_coa_svm, average='weighted')
confusion_train_b_coa_svm = confusion_matrix(y_train_b_coa_svm, y_pred_train_b_coa_svm)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_b_coa_svm)
print("Precision: ", precision_train_b_coa_svm)
print("Precision per class:" , precision_x_class_train_b_coa_svm )
print("Recall: ", recall_train_b_coa_svm)
print("Recall per class:" , recall_x_class_train_b_coa_svm )
print("F1 Score: ", f1_score_train_b_coa_svm)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_b_coa_svm):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_b_coa_svm )
print("Confusion Matrix: ")
print(confusion_train_b_coa_svm)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_b_coa_svm = best_model_b_coa_svm.predict(X_test_b_coa_svm)

# Evaluation metrics on the testing set
accuracy_test_b_coa_svm = accuracy_score(y_test_b_coa_svm, y_pred_test_b_coa_svm)
precision_test_b_coa_svm = precision_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average='weighted')
# Calculate precision for each class
precision_x_class_test_b_coa_svm = precision_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average=None)
recall_test_b_coa_svm = recall_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average='weighted')
# Calculate recall for each class
recall_x_class_test_b_coa_svm = recall_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average=None)
f1_score_test_b_coa_svm = f1_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_b_coa_svm = f1_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average=None)
jaccard_test_b_coa_svm = jaccard_score(y_test_b_coa_svm, y_pred_test_b_coa_svm, average='weighted')
confusion_test_b_coa_svm = confusion_matrix(y_test_b_coa_svm, y_pred_test_b_coa_svm)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_b_coa_svm)
print("Precision: ", precision_test_b_coa_svm)
print("Precision per class:" , precision_x_class_test_b_coa_svm )
print("Recall: ", recall_test_b_coa_svm)
print("Recall per class:" , recall_x_class_test_b_coa_svm )
print("F1 Score: ", f1_score_test_b_coa_svm)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_b_coa_svm):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_b_coa_svm )
print("Confusion Matrix: ")
print(confusion_test_b_coa_svm)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_b_coa_svm = scaler_b_coa_svm.transform(X_val_coastal)

# Make predictions on the validation set using the trained svm model
y_pred_val_b_coa_svm = best_model_b_coa_svm.predict(X_val_b_coa_svm)

# Evaluation metrics on the validation set
accuracy_val_b_coa_svm = accuracy_score(y_val_coastal, y_pred_val_b_coa_svm)
precision_val_b_coa_svm = precision_score(y_val_coastal, y_pred_val_b_coa_svm, average='weighted')
# Calculate precision for each class
precision_x_class_val_b_coa_svm = precision_score(y_val_coastal, y_pred_val_b_coa_svm, average=None)
recall_val_b_coa_svm = recall_score(y_val_coastal, y_pred_val_b_coa_svm, average='weighted')
# Calculate recall for each class
recall_x_class_val_b_coa_svm = recall_score(y_val_coastal, y_pred_val_b_coa_svm, average=None)
f1_score_val_b_coa_svm = f1_score(y_val_coastal, y_pred_val_b_coa_svm, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_b_coa_svm = f1_score(y_val_coastal, y_pred_val_b_coa_svm, average=None)
jaccard_val_b_coa_svm = jaccard_score(y_val_coastal, y_pred_val_b_coa_svm, average='weighted')
confusion_val_b_coa_svm = confusion_matrix(y_val_coastal, y_pred_val_b_coa_svm)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_b_coa_svm)
print("Precision: ", precision_val_b_coa_svm)
print("Precision per class:" , precision_x_class_val_b_coa_svm )
print("Recall: ", recall_val_b_coa_svm)
print("Recall per class:" , recall_x_class_val_b_coa_svm )
print("F1 Score: ", f1_score_val_b_coa_svm)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_b_coa_svm):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_b_coa_svm )
print("Confusion Matrix: ")
print(confusion_val_b_coa_svm)

# Save the trained SVM model to a file
with open('best_model_b_coa_svm.pkl', 'wb') as model_file:
    pickle.dump(best_model_b_coa_svm, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_b_coa_svm.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_b_coa_svm, scaler_file)

"""###KNN

Split data into training and test set
"""

X_train_b_coa_knn, X_test_b_coa_knn, y_train_b_coa_knn, y_test_b_coa_knn = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_b_coa_knn = StandardScaler()

# Fit and transform the training data to standardize features
X_train_b_coa_knn = scaler_b_coa_knn.fit_transform(X_train_b_coa_knn)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_b_coa_knn = scaler_b_coa_knn.transform(X_test_b_coa_knn)

"""Hyperparameter Grid for knn Model"""

# Define a dictionary of hyperparameters for tuning a KNN model
parameters = {
    'n_neighbors': [3, 5, 7, 9],  # Número de vecinos
    'weights': ['uniform', 'distance'],  # Tipo de ponderación
    'p': [1, 2]  # Parámetro para la distancia (1 para la distancia de Manhattan, 2 para la Euclidiana)
}

"""Create a Decision knn model"""

model_b_coa_knn = KNeighborsClassifier()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_b_coa_knn = GridSearchCV(model_b_coa_knn, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_b_coa_knn.fit(X_train_b_coa_knn, y_train_b_coa_knn)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_b_coa_knn = grid_search_b_coa_knn.best_estimator_
print(best_model_b_coa_knn)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_b_coa_knn = grid_search_b_coa_knn.best_params_
print(best_params_b_coa_knn)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_b_coa_knn = grid_search_b_coa_knn.best_score_
print(best_score_b_coa_knn)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_b_coa_knn = best_model_b_coa_knn.predict(X_train_b_coa_knn)

# Evaluation metrics on the training set
accuracy_train_b_coa_knn = accuracy_score(y_train_b_coa_knn, y_pred_train_b_coa_knn)
precision_train_b_coa_knn = precision_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average='weighted')
# Calculate precision for each class
precision_x_class_train_b_coa_knn = precision_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average=None)
recall_train_b_coa_knn = recall_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average='weighted')
# Calculate recall for each class
recall_x_class_train_b_coa_knn = recall_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average=None)
f1_score_train_b_coa_knn = f1_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_b_coa_knn = f1_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average=None)
jaccard_train_b_coa_knn = jaccard_score(y_train_b_coa_knn, y_pred_train_b_coa_knn, average='weighted')
confusion_train_b_coa_knn = confusion_matrix(y_train_b_coa_knn, y_pred_train_b_coa_knn)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_b_coa_knn)
print("Precision: ", precision_train_b_coa_knn)
print("Precision per class:" , precision_x_class_train_b_coa_knn )
print("Recall: ", recall_train_b_coa_knn)
print("Recall per class:" , recall_x_class_train_b_coa_knn )
print("F1 Score: ", f1_score_train_b_coa_knn)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_b_coa_knn):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_b_coa_knn )
print("Confusion Matrix: ")
print(confusion_train_b_coa_knn)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_b_coa_knn = best_model_b_coa_knn.predict(X_test_b_coa_knn)

# Evaluation metrics on the testing set
accuracy_test_b_coa_knn = accuracy_score(y_test_b_coa_knn, y_pred_test_b_coa_knn)
precision_test_b_coa_knn = precision_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average='weighted')
# Calculate precision for each class
precision_x_class_test_b_coa_knn = precision_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average=None)
recall_test_b_coa_knn = recall_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average='weighted')
# Calculate recall for each class
recall_x_class_test_b_coa_knn = recall_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average=None)
f1_score_test_b_coa_knn = f1_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_b_coa_knn = f1_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average=None)
jaccard_test_b_coa_knn = jaccard_score(y_test_b_coa_knn, y_pred_test_b_coa_knn, average='weighted')
confusion_test_b_coa_knn = confusion_matrix(y_test_b_coa_knn, y_pred_test_b_coa_knn)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_b_coa_knn)
print("Precision: ", precision_test_b_coa_knn)
print("Precision per class:" , precision_x_class_test_b_coa_knn )
print("Recall: ", recall_test_b_coa_knn)
print("Recall per class:" , recall_x_class_test_b_coa_knn )
print("F1 Score: ", f1_score_test_b_coa_knn)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_b_coa_knn):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_b_coa_knn )
print("Confusion Matrix: ")
print(confusion_test_b_coa_knn)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_b_coa_knn = scaler_b_coa_knn.transform(X_val_coastal)

# Make predictions on the validation set using the trained knn model
y_pred_val_b_coa_knn = best_model_b_coa_knn.predict(X_val_b_coa_knn)

# Evaluation metrics on the validation set
accuracy_val_b_coa_knn = accuracy_score(y_val_coastal, y_pred_val_b_coa_knn)
precision_val_b_coa_knn = precision_score(y_val_coastal, y_pred_val_b_coa_knn, average='weighted')
# Calculate precision for each class
precision_x_class_val_b_coa_knn = precision_score(y_val_coastal, y_pred_val_b_coa_knn, average=None)
recall_val_b_coa_knn = recall_score(y_val_coastal, y_pred_val_b_coa_knn, average='weighted')
# Calculate recall for each class
recall_x_class_val_b_coa_knn = recall_score(y_val_coastal, y_pred_val_b_coa_knn, average=None)
f1_score_val_b_coa_knn = f1_score(y_val_coastal, y_pred_val_b_coa_knn, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_b_coa_knn = f1_score(y_val_coastal, y_pred_val_b_coa_knn, average=None)
jaccard_val_b_coa_knn = jaccard_score(y_val_coastal, y_pred_val_b_coa_knn, average='weighted')
confusion_val_b_coa_knn = confusion_matrix(y_val_coastal, y_pred_val_b_coa_knn)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_b_coa_knn)
print("Precision: ", precision_val_b_coa_knn)
print("Precision per class:" , precision_x_class_val_b_coa_knn)
print("Recall: ", recall_val_b_coa_knn)
print("Recall per class:" , recall_x_class_val_b_coa_knn)
print("F1 Score: ", f1_score_val_b_coa_knn)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_b_coa_knn):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_b_coa_knn)
print("Confusion Matrix: ")
print(confusion_val_b_coa_knn)

# Save the trained XGBoost model to a file
with open('best_model_b_coa_knn.pkl', 'wb') as model_file:
    pickle.dump(best_model_b_coa_knn, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_b_coa_knn.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_b_coa_knn, scaler_file)

"""##DT

Split data into training and test set
"""

X_train_b_coa_dt, X_test_b_coa_dt, y_train_b_coa_dt, y_test_b_coa_dt = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_b_coa_dt = StandardScaler()

# Fit and transform the training data to standardize features
X_train_b_coa_dt = scaler_b_coa_dt.fit_transform(X_train_b_coa_dt)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_b_coa_dt = scaler_b_coa_dt.transform(X_test_b_coa_dt)

"""Hyperparameter Grid for XGBoost Model"""

# Define a dictionary of hyperparameters for tuning a DT model
parameters = {
    'criterion': ['gini', 'entropy'],  # Criterio para la división de nodos
    'max_depth': [None, 10, 20, 30, 40, 50],  # Profundidad máxima del árbol
    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras para dividir un nodo
    'min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en una hoja
}

"""Create a Decision DT model"""

model_b_coa_dt = DecisionTreeClassifier()

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_b_coa_dt = GridSearchCV(model_b_coa_dt, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_b_coa_dt.fit(X_train_b_coa_dt, y_train_b_coa_dt)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_b_coa_dt = grid_search_b_coa_dt.best_estimator_
print(best_model_b_coa_dt)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_b_coa_dt = grid_search_b_coa_dt.best_params_
print(best_params_b_coa_dt)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_b_coa_dt = grid_search_b_coa_dt.best_score_
print(best_score_b_coa_dt)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_b_coa_dt = best_model_b_coa_dt.predict(X_train_b_coa_dt)

# Evaluation metrics on the training set
accuracy_train_b_coa_dt = accuracy_score(y_train_b_coa_dt, y_pred_train_b_coa_dt)
precision_train_b_coa_dt = precision_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average='weighted')
# Calculate precision for each class
precision_x_class_train_b_coa_dt = precision_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average=None)
recall_train_b_coa_dt = recall_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average='weighted')
# Calculate recall for each class
recall_x_class_train_b_coa_dt = recall_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average=None)
f1_score_train_b_coa_dt = f1_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_b_coa_dt = f1_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average=None)
jaccard_train_b_coa_dt = jaccard_score(y_train_b_coa_dt, y_pred_train_b_coa_dt, average='weighted')
confusion_train_b_coa_dt = confusion_matrix(y_train_b_coa_dt, y_pred_train_b_coa_dt)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_b_coa_dt)
print("Precision: ", precision_train_b_coa_dt)
print("Precision per class:" , precision_x_class_train_b_coa_dt )
print("Recall: ", recall_train_b_coa_dt)
print("Recall per class:" , recall_x_class_train_b_coa_dt )
print("F1 Score: ", f1_score_train_b_coa_dt)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_b_coa_dt):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_b_coa_dt )
print("Confusion Matrix: ")
print(confusion_train_b_coa_dt)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_b_coa_dt = best_model_b_coa_dt.predict(X_test_b_coa_dt)

# Evaluation metrics on the test set
accuracy_test_b_coa_dt = accuracy_score(y_test_b_coa_dt, y_pred_test_b_coa_dt)
precision_test_b_coa_dt = precision_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average='weighted')
# Calculate precision for each class
precision_x_class_test_b_coa_dt = precision_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average=None)
recall_test_b_coa_dt = recall_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average='weighted')
# Calculate recall for each class
recall_x_class_test_b_coa_dt = recall_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average=None)
f1_score_test_b_coa_dt = f1_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_b_coa_dt = f1_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average=None)
jaccard_test_b_coa_dt = jaccard_score(y_test_b_coa_dt, y_pred_test_b_coa_dt, average='weighted')
confusion_test_b_coa_dt = confusion_matrix(y_test_b_coa_dt, y_pred_test_b_coa_dt)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_b_coa_dt)
print("Precision: ", precision_test_b_coa_dt)
print("Precision per class:" , precision_x_class_test_b_coa_dt )
print("Recall: ", recall_test_b_coa_dt)
print("Recall per class:" , recall_x_class_test_b_coa_dt )
print("F1 Score: ", f1_score_test_b_coa_dt)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_b_coa_dt):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_b_coa_dt )
print("Confusion Matrix: ")
print(confusion_test_b_coa_dt)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_b_coa_dt = scaler_b_coa_dt.transform(X_val_coastal)

# Make predictions on the validation set using the trained dt model
y_pred_val_b_coa_dt = best_model_b_coa_dt.predict(X_val_b_coa_dt)

# Evaluation metrics on the validation set
accuracy_val_b_coa_dt = accuracy_score(y_val_coastal, y_pred_val_b_coa_dt)
precision_val_b_coa_dt = precision_score(y_val_coastal, y_pred_val_b_coa_dt, average='weighted')
# Calculate precision for each class
precision_x_class_val_b_coa_dt = precision_score(y_val_coastal, y_pred_val_b_coa_dt, average=None)
recall_val_b_coa_dt = recall_score(y_val_coastal, y_pred_val_b_coa_dt, average='weighted')
# Calculate recall for each class
recall_x_class_val_b_coa_dt = recall_score(y_val_coastal, y_pred_val_b_coa_dt, average=None)
f1_score_val_b_coa_dt = f1_score(y_val_coastal, y_pred_val_b_coa_dt, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_b_coa_dt = f1_score(y_val_coastal, y_pred_val_b_coa_dt, average=None)
jaccard_val_b_coa_dt = jaccard_score(y_val_coastal, y_pred_val_b_coa_dt, average='weighted')
confusion_val_b_coa_dt = confusion_matrix(y_val_coastal, y_pred_val_b_coa_dt)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_b_coa_dt)
print("Precision: ", precision_val_b_coa_dt)
print("Precision per class:" , precision_x_class_val_b_coa_dt )
print("Recall: ", recall_val_b_coa_dt)
print("Recall per class:" , recall_x_class_val_b_coa_dt )
print("F1 Score: ", f1_score_val_b_coa_dt)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_b_coa_dt):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_b_coa_dt )
print("Confusion Matrix: ")
print(confusion_val_b_coa_dt)

# Save the trained SVM model to a file
with open('best_model_b_coa_dt.pkl', 'wb') as model_file:
    pickle.dump(best_model_b_coa_dt, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_b_coa_dt.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_b_coa_dt, scaler_file)

"""##MLR

Split data into training and test set
"""

X_train_b_coa_mlr, X_test_b_coa_mlr, y_train_b_coa_mlr, y_test_b_coa_mlr = train_test_split(X_train_coastal, y_train_coastal, test_size=0.2, random_state=42)

"""Feature Standardization for coastal Dataset using StandardScaler"""

# Create a StandardScaler object
scaler_b_coa_mlr = StandardScaler()

# Fit and transform the training data to standardize features
X_train_b_coa_mlr = scaler_b_coa_mlr.fit_transform(X_train_b_coa_mlr)

# Transform the test data using the same scaler to maintain consistency in feature scaling
X_test_b_coa_mlr = scaler_b_coa_mlr.transform(X_test_b_coa_mlr)

"""Hyperparameter Grid for MLR Model"""

# Define a dictionary of hyperparameters for tuning a MLR model
parameters = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Parámetro de regularización
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  # Método de optimización
}

"""Create a Decision MLR model"""

model_b_coa_mlr = LogisticRegression(max_iter=1000)  # Usamos max_iter para evitar advertencias

"""Hyperparameter Tuning with Cross-Validation"""

# Inform the user to wait during the hyperparameter search
print("Wait a moment...")

# Create a GridSearchCV object for hyperparameter tuning
grid_search_b_coa_mlr = GridSearchCV(model_b_coa_mlr, parameters, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to find the best hyperparameters
grid_search_b_coa_mlr.fit(X_train_b_coa_mlr, y_train_b_coa_mlr)

"""Best Estimator and Hyperparameters"""

# Print the best estimator found during hyperparameter search
print("Best estimator found:")
best_model_b_coa_mlr = grid_search_b_coa_mlr.best_estimator_
print(best_model_b_coa_mlr)

# Print the best hyperparameters found during hyperparameter search
print("\nBest hyperparameters found:")
best_params_b_coa_mlr = grid_search_b_coa_mlr.best_params_
print(best_params_b_coa_mlr)

# Print the score of the best model in coastal_no_balancing cross-validation
print("\nScore of the best model in coastal_no_balancing cross-validation:")
best_score_b_coa_mlr = grid_search_b_coa_mlr.best_score_
print(best_score_b_coa_mlr)

""" Evaluate the model on the training set"""

# Make predictions on the training set
y_pred_train_b_coa_mlr = best_model_b_coa_mlr.predict(X_train_b_coa_mlr)

# Evaluation metrics on the training set
accuracy_train_b_coa_mlr = accuracy_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr)
precision_train_b_coa_mlr = precision_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average='weighted')
# Calculate precision for each class
precision_x_class_train_b_coa_mlr = precision_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average=None)
recall_train_b_coa_mlr = recall_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average='weighted')
# Calculate recall for each class
recall_x_class_train_b_coa_mlr = recall_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average=None)
f1_score_train_b_coa_mlr = f1_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_train_b_coa_mlr = f1_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average=None)
jaccard_train_b_coa_mlr = jaccard_score(y_train_b_coa_mlr, y_pred_train_b_coa_mlr, average='weighted')
confusion_train_b_coa_mlr = confusion_matrix(y_train_b_coa_mlr, y_pred_train_b_coa_mlr)

# Print evaluation metrics for the training set
print("Accuracy: ", accuracy_train_b_coa_mlr)
print("Precision: ", precision_train_b_coa_mlr)
print("Precision per class:" , precision_x_class_train_b_coa_mlr )
print("Recall: ", recall_train_b_coa_mlr)
print("Recall per class:" , recall_x_class_train_b_coa_mlr )
print("F1 Score: ", f1_score_train_b_coa_mlr)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_train_b_coa_mlr):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_train_b_coa_mlr )
print("Confusion Matrix: ")
print(confusion_train_b_coa_mlr)

""" Evaluate the model on the test set"""

# Make predictions on the testing set
y_pred_test_b_coa_mlr = best_model_b_coa_mlr.predict(X_test_b_coa_mlr)

# Evaluation metrics on the testing set
accuracy_test_b_coa_mlr = accuracy_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr)
precision_test_b_coa_mlr = precision_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average='weighted')
# Calculate precision for each class
precision_x_class_test_b_coa_mlr = precision_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average=None)
recall_test_b_coa_mlr = recall_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average='weighted')
# Calculate recall for each class
recall_x_class_test_b_coa_mlr = recall_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average=None)
f1_score_test_b_coa_mlr = f1_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_test_b_coa_mlr = f1_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average=None)
jaccard_test_b_coa_mlr = jaccard_score(y_test_b_coa_mlr, y_pred_test_b_coa_mlr, average='weighted')
confusion_test_b_coa_mlr = confusion_matrix(y_test_b_coa_mlr, y_pred_test_b_coa_mlr)

# Print evaluation metrics for the testing set
print("Accuracy: ", accuracy_test_b_coa_mlr)
print("Precision: ", precision_test_b_coa_mlr)
print("Precision per class:" , precision_x_class_test_b_coa_mlr )
print("Recall: ", recall_test_b_coa_mlr)
print("Recall per class:" , recall_x_class_test_b_coa_mlr )
print("F1 Score: ", f1_score_test_b_coa_mlr)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_test_b_coa_mlr):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_test_b_coa_mlr )
print("Confusion Matrix: ")
print(confusion_test_b_coa_mlr)

""" Evaluate the model on the validation set"""

# Transform the validation data using the scaler trained on the validation data
X_val_b_coa_mlr = scaler_b_coa_mlr.transform(X_val_coastal)

# Make predictions on the validation set using the trained MLR model
y_pred_val_b_coa_mlr = best_model_b_coa_mlr.predict(X_val_b_coa_mlr)

# Evaluation metrics on the validation set
accuracy_val_b_coa_mlr = accuracy_score(y_val_coastal, y_pred_val_b_coa_mlr)
precision_val_b_coa_mlr = precision_score(y_val_coastal, y_pred_val_b_coa_mlr, average='weighted')
# Calculate precision for each class
precision_x_class_val_b_coa_mlr = precision_score(y_val_coastal, y_pred_val_b_coa_mlr, average=None)
recall_val_b_coa_mlr = recall_score(y_val_coastal, y_pred_val_b_coa_mlr, average='weighted')
# Calculate recall for each class
recall_x_class_val_b_coa_mlr = recall_score(y_val_coastal, y_pred_val_b_coa_mlr, average=None)
f1_score_val_b_coa_mlr = f1_score(y_val_coastal, y_pred_val_b_coa_mlr, average='weighted')
# Calculate f1 score for each class
f1_score_x_class_val_b_coa_mlr = f1_score(y_val_coastal, y_pred_val_b_coa_mlr, average=None)
jaccard_val_b_coa_mlr = jaccard_score(y_val_coastal, y_pred_val_b_coa_mlr, average='weighted')
confusion_val_b_coa_mlr = confusion_matrix(y_val_coastal, y_pred_val_b_coa_mlr)

# Print evaluation metrics for the validation set
print("Accuracy: ", accuracy_val_b_coa_mlr)
print("Precision: ", precision_val_b_coa_mlr)
print("Precision per class:" , precision_x_class_val_b_coa_mlr )
print("Recall: ", recall_val_b_coa_mlr)
print("Recall per class:" , recall_x_class_val_b_coa_mlr )
print("F1 Score: ", f1_score_val_b_coa_mlr)
# Print F1-score for each class
for clase, f1 in enumerate(f1_score_x_class_val_b_coa_mlr):
    print(f"F1-score for the class {clase}: {f1}")
print("jaccard:" , jaccard_val_b_coa_mlr )
print("Confusion Matrix: ")
print(confusion_val_b_coa_mlr)

# Save the trained MLR model to a file
with open('best_model_b_coa_mlr.pkl', 'wb') as model_file:
    pickle.dump(best_model_b_coa_mlr, model_file)

# Save the scaler used for preprocessing to a file
with open('scaler_b_coa_mlr.pkl', 'wb') as scaler_file:
    pickle.dump(scaler_b_coa_mlr, scaler_file)